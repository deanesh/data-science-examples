{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6ffc78",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### üß† Intermediate Machine Learning\n",
    "\n",
    "This course prepares you to **build more accurate and robust models** by tackling common data issues and exploring more advanced techniques.\n",
    "\n",
    "---\n",
    "\n",
    "###### üìò Course Breakdown\n",
    "\n",
    "##### 1. **Introduction**\n",
    "\n",
    "* Quick recap of what you've learned so far (fitting models, validation, random forests).\n",
    "* Overview of **key real-world challenges**:\n",
    "\n",
    "  * Missing values\n",
    "  * Categorical variables\n",
    "  * Feature engineering\n",
    "  * Model tuning\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **Handling Missing Values**\n",
    "\n",
    "Real-world data often has **missing entries** (NaNs). This lesson covers:\n",
    "\n",
    "* How to **detect** missing values\n",
    "* Strategies to handle them:\n",
    "\n",
    "  * Drop rows/columns\n",
    "  * **Imputation** (filling in with mean, median, or constant)\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_X = pd.DataFrame(imputer.fit_transform(X))\n",
    "imputed_X.columns = X.columns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **Handling Categorical Variables**\n",
    "\n",
    "Models can‚Äôt directly work with text (categorical) data. You‚Äôll learn two main methods:\n",
    "\n",
    "* **Label Encoding**: Assigns a number to each category\n",
    "\n",
    "  * Good for **ordinal** (ordered) data\n",
    "* **One-Hot Encoding**: Creates binary columns for each category\n",
    "\n",
    "  * Good for **nominal** (unordered) data\n",
    "\n",
    "```python\n",
    "# One-Hot Encoding\n",
    "X = pd.get_dummies(X)\n",
    "```\n",
    "\n",
    "You‚Äôll also learn how to align encoded train/test data so the model doesn‚Äôt fail.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **Pipelines**\n",
    "\n",
    "Pipelines let you **bundle preprocessing and modeling** steps together. This helps:\n",
    "\n",
    "* Keep code cleaner\n",
    "* Avoid data leakage\n",
    "* Automate preprocessing during predictions\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **Cross-Validation**\n",
    "\n",
    "Instead of a single train/test split, **cross-validation** helps you better estimate your model‚Äôs performance by training and testing it multiple times on different splits of the data.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### 6. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "A **powerful ML algorithm** that often outperforms Random Forests in structured/tabular data.\n",
    "\n",
    "* Based on gradient boosting trees\n",
    "* Requires conversion of data to numeric format\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "You‚Äôll also learn how to use:\n",
    "\n",
    "* Early stopping\n",
    "* Parameter tuning for XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "##### 7. **Data Leakage**\n",
    "\n",
    "One of the most important real-world ML concepts.\n",
    "\n",
    "**Data leakage** happens when the model accidentally learns from information it **shouldn‚Äôt have access to during training**, usually leading to **over-optimistic results**.\n",
    "\n",
    "You‚Äôll learn:\n",
    "\n",
    "* How to detect leakage\n",
    "* How to prevent it by properly splitting data and excluding features\n",
    "\n",
    "---\n",
    "\n",
    "##### üõ†Ô∏è Tools Used\n",
    "\n",
    "* **Python**\n",
    "* **pandas**\n",
    "* **scikit-learn**\n",
    "* **XGBoost**\n",
    "* Kaggle **Notebooks**\n",
    "\n",
    "\n",
    "##### ‚úÖ Skills You'll Gain\n",
    "\n",
    "* Handle missing and categorical data like a pro\n",
    "* Build clean, reusable pipelines\n",
    "* Use cross-validation for better model evaluation\n",
    "* Train advanced models like XGBoost\n",
    "* Avoid common pitfalls like data leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671012a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

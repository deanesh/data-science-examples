{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac9a223",
   "metadata": {},
   "source": [
    "#### 🧠 **What is Computer Vision?**\n",
    "\n",
    "**Computer Vision** is a field of AI that enables computers to \"see\" and interpret images. It's behind technologies like:\n",
    "\n",
    "* Facial recognition\n",
    "* Object detection\n",
    "* Medical image analysis\n",
    "* Self-driving cars\n",
    "\n",
    "#### 1. **Welcome to Computer Vision**\n",
    "\n",
    "* Overview of what CV is and why CNNs are used\n",
    "* Introduction to **convolutional layers** and how they detect patterns in images\n",
    "\n",
    "📌 You’ll learn:\n",
    "\n",
    "* Why regular dense layers don’t work well on images\n",
    "* What a **convolution** does in a neural network\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "* Build your **first CNN** using Keras\n",
    "* Compare it with a standard dense network\n",
    "\n",
    "📌 Key Concepts:\n",
    "\n",
    "* **Conv2D** layers\n",
    "* **MaxPooling2D**\n",
    "* How CNNs learn to detect edges, textures, and shapes\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Using Real-World Images**\n",
    "\n",
    "* Load and preprocess real image data using Keras utilities\n",
    "* Use **ImageDataGenerator** to read images from directories and apply real-time data augmentation\n",
    "\n",
    "📌 You’ll learn:\n",
    "\n",
    "* Image resizing\n",
    "* Color channel handling (RGB, grayscale)\n",
    "* Data augmentation (rotation, flip, zoom)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_data = train_gen.flow_from_directory('train/', target_size=(150, 150))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Data Augmentation**\n",
    "\n",
    "* Prevent overfitting by artificially increasing training data\n",
    "* Use built-in `ImageDataGenerator` parameters:\n",
    "\n",
    "  * `rotation_range`\n",
    "  * `width_shift_range`\n",
    "  * `zoom_range`, etc.\n",
    "\n",
    "📌 This helps models generalize better by simulating real-world variability in images.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Transfer Learning**\n",
    "\n",
    "* Use **pre-trained models** like VGG16 or MobileNet\n",
    "* Great for when you have **limited data** but want **high accuracy**\n",
    "* Freeze lower layers and train only the final layers\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "```\n",
    "\n",
    "You’ll learn how to:\n",
    "\n",
    "* Use the pre-trained model as a **feature extractor**\n",
    "* Add your own custom classifier on top\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Interpret Model Predictions**\n",
    "\n",
    "* Learn to **visualize what the model is seeing**\n",
    "* Use tools like:\n",
    "\n",
    "  * **Confusion matrix**\n",
    "  * **Class activation maps (CAMs)**\n",
    "* Evaluate how confident the model is in its predictions\n",
    "\n",
    "  📌 Helps you debug and improve models — especially in **high-stakes domains** like medical imaging.\n",
    "---\n",
    "\n",
    "#### ✅ Skills You'll Gain\n",
    "\n",
    "* Build and train **CNNs** from scratch\n",
    "* Preprocess and augment **real-world images**\n",
    "* Use **transfer learning** to improve model performance\n",
    "* Visualize model predictions and evaluate performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55018f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebce418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=64, activation=\"relu\"),\n",
    "        layers.Dropout(rate=0.3),\n",
    "        layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e67dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "\n",
    "train_data_generator = train_generator.flow_from_directory(\n",
    "    \"../data/train\", target_size=(150, 150), batch_size=32, class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 4 classes.\n",
      "Found 9 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Git-Repos\\data-science-examples\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3333 - loss: 2.0139 - val_accuracy: 0.4444 - val_loss: 1.1405\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 1.0583 - val_accuracy: 1.0000 - val_loss: 0.1657\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0970 - val_accuracy: 0.8889 - val_loss: 0.1793\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0782 - val_accuracy: 1.0000 - val_loss: 0.0970\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7778 - loss: 0.5599 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0316 - val_accuracy: 1.0000 - val_loss: 0.0052\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 0.0106\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8889 - loss: 0.1324 - val_accuracy: 1.0000 - val_loss: 0.0096\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x204faccd2b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_model = VGG16(weights=\"imagenet\", input_shape=(150, 150, 3), include_top=False)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation=\"softmax\"),  # For binary classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(metrics=[\"accuracy\"], optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "val_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    \"../data/train\", target_size=(150, 150), batch_size=32, class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_data = val_gen.flow_from_directory(\n",
    "    \"../data/validation\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf5b8a",
   "metadata": {},
   "source": [
    "**GRU (Gated Recurrent Unit)** is a type of **RNN** that, like **LSTM**, is designed to **handle long-term dependencies** in sequential data, but with a **simpler structure**.\n",
    "\n",
    "#### In short:\n",
    "> **GRU** = A simpler, faster alternative to LSTM that still captures long-term patterns in sequences.\n",
    "#### Key features:\n",
    "\n",
    "* Uses **two gates**: **update gate** and **reset gate**.\n",
    "* No separate memory cell (unlike LSTM).\n",
    "* Fewer parameters → **faster training** and often similar performance.\n",
    "\n",
    "GRUs are widely used in tasks like **text generation, machine translation, and speech recognition**, especially when efficiency is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c371c6",
   "metadata": {},
   "source": [
    "**RNN (Recurrent Neural Network)** is a type of neural network designed to **process sequential data**, such as time series, text, or speech.\n",
    "\n",
    "### In short:\n",
    "\n",
    "> **RNN** = Neural network that **remembers past information** using loops in its architecture.\n",
    "\n",
    "### Key features:\n",
    "\n",
    "* Has a **\"memory\"** of previous inputs (via hidden states).\n",
    "* Processes input **one step at a time**, maintaining context across the sequence.\n",
    "* Commonly used in **language modeling, speech recognition, and time-series forecasting**.\n",
    "\n",
    "However, standard RNNs struggle with long sequences due to **vanishing gradients**, so improved versions like **LSTM** and **GRU** are often used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7122c02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

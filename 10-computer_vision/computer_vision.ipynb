{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac9a223",
   "metadata": {},
   "source": [
    "#### 🧠 **What is Computer Vision?**\n",
    "\n",
    "**Computer Vision** is a field of AI that enables computers to \"see\" and interpret images. It's behind technologies like:\n",
    "\n",
    "* Facial recognition\n",
    "* Object detection\n",
    "* Medical image analysis\n",
    "* Self-driving cars\n",
    "\n",
    "#### 1. **Welcome to Computer Vision**\n",
    "\n",
    "* Overview of what CV is and why CNNs are used\n",
    "* Introduction to **convolutional layers** and how they detect patterns in images\n",
    "\n",
    "📌 You’ll learn:\n",
    "\n",
    "* Why regular dense layers don’t work well on images\n",
    "* What a **convolution** does in a neural network\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "* Build your **first CNN** using Keras\n",
    "* Compare it with a standard dense network\n",
    "\n",
    "📌 Key Concepts:\n",
    "\n",
    "* **Conv2D** layers\n",
    "* **MaxPooling2D**\n",
    "* How CNNs learn to detect edges, textures, and shapes\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Using Real-World Images**\n",
    "\n",
    "* Load and preprocess real image data using Keras utilities\n",
    "* Use **ImageDataGenerator** to read images from directories and apply real-time data augmentation\n",
    "\n",
    "📌 You’ll learn:\n",
    "\n",
    "* Image resizing\n",
    "* Color channel handling (RGB, grayscale)\n",
    "* Data augmentation (rotation, flip, zoom)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_data = train_gen.flow_from_directory('train/', target_size=(150, 150))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Data Augmentation**\n",
    "\n",
    "* Prevent overfitting by artificially increasing training data\n",
    "* Use built-in `ImageDataGenerator` parameters:\n",
    "\n",
    "  * `rotation_range`\n",
    "  * `width_shift_range`\n",
    "  * `zoom_range`, etc.\n",
    "\n",
    "📌 This helps models generalize better by simulating real-world variability in images.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Transfer Learning**\n",
    "\n",
    "* Use **pre-trained models** like VGG16 or MobileNet\n",
    "* Great for when you have **limited data** but want **high accuracy**\n",
    "* Freeze lower layers and train only the final layers\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "```\n",
    "\n",
    "You’ll learn how to:\n",
    "\n",
    "* Use the pre-trained model as a **feature extractor**\n",
    "* Add your own custom classifier on top\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Interpret Model Predictions**\n",
    "\n",
    "* Learn to **visualize what the model is seeing**\n",
    "* Use tools like:\n",
    "\n",
    "  * **Confusion matrix**\n",
    "  * **Class activation maps (CAMs)**\n",
    "* Evaluate how confident the model is in its predictions\n",
    "\n",
    "  📌 Helps you debug and improve models — especially in **high-stakes domains** like medical imaging.\n",
    "---\n",
    "\n",
    "#### ✅ Skills You'll Gain\n",
    "\n",
    "* Build and train **CNNs** from scratch\n",
    "* Preprocess and augment **real-world images**\n",
    "* Use **transfer learning** to improve model performance\n",
    "* Visualize model predictions and evaluate performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55018f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebce418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_1, built=True>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=64, activation=\"relu\"),\n",
    "        layers.Dropout(rate=0.3),\n",
    "        layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acuracy\"]\n",
    ")\n",
    "\n",
    "# model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e67dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.legacy.preprocessing.image.DirectoryIterator at 0x247c1392210>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "\n",
    "train_data_generator = train_generator.flow_from_directory(\n",
    "    \"../data/train\", target_size=(150, 150), batch_size=32, class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "\n",
    "train_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3394a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 4 classes.\n",
      "Found 9 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.1111 - loss: 1.9159 - val_accuracy: 0.5556 - val_loss: 1.0125\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5556 - loss: 0.9075 - val_accuracy: 0.7778 - val_loss: 0.3690\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8889 - loss: 0.5479 - val_accuracy: 1.0000 - val_loss: 0.0718\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.1131 - val_accuracy: 1.0000 - val_loss: 0.0187\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8889 - loss: 0.4076 - val_accuracy: 1.0000 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0208 - val_accuracy: 1.0000 - val_loss: 0.0380\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0605\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8889 - loss: 0.3580 - val_accuracy: 1.0000 - val_loss: 0.0302\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8889 - loss: 0.4714 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0657 - val_accuracy: 1.0000 - val_loss: 2.6515e-04\n",
      "{'Car': 0, 'Truck': 1, 'man': 2, 'women': 3}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_model = VGG16(weights=\"imagenet\", input_shape=(150, 150, 3), include_top=False)\n",
    "base_model\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation=\"softmax\"),  # For binary classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(metrics=[\"accuracy\"], optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "val_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    \"../data/train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",  # one-hot encoded labels\n",
    ")\n",
    "\n",
    "val_data = val_gen.flow_from_directory(\n",
    "    \"../data/validation\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf5b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

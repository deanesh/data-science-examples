{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac9a223",
   "metadata": {},
   "source": [
    "#### ðŸ§  What is Feature Engineering?\n",
    "\n",
    "**Feature engineering** is the process of:\n",
    "\n",
    "* Selecting the most relevant variables (features)\n",
    "* Creating new features from existing ones\n",
    "* Transforming features to be more suitable for machine learning models\n",
    "\n",
    "Itâ€™s the art of **making your data more understandable to the model**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“˜ What Youâ€™ll Learn in This Course\n",
    "\n",
    "#### 1. **Introduction to Feature Engineering**\n",
    "\n",
    "* What is a **feature**?\n",
    "* Why features matter more than the model in many cases.\n",
    "* Introduction to techniques for:\n",
    "\n",
    "  * Creating features\n",
    "  * Transforming features\n",
    "  * Selecting features\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Baseline Model**\n",
    "\n",
    "* Create a **baseline model** using raw features to compare improvements later.\n",
    "* Example: Use a simple **Random Forest** without feature transformations.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Better Features for Tree Models**\n",
    "\n",
    "* Tree-based models (like Random Forests, XGBoost) donâ€™t need feature scaling.\n",
    "* Youâ€™ll learn:\n",
    "\n",
    "  * How to handle **ordinal** vs **nominal** data\n",
    "  * Creating **interaction features** (e.g. multiplying or combining two columns)\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "data['price_per_sqft'] = data['price'] / data['square_feet']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Numeric Transformations**\n",
    "\n",
    "* Transform numeric features to improve model performance:\n",
    "\n",
    "  * **Log transforms** to reduce skew\n",
    "  * **Scaling** (though less useful for trees)\n",
    "  * Handling outliers\n",
    "  * **Clipping** extreme values\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data['log_income'] = np.log(data['income'] + 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Categorical Variables**\n",
    "\n",
    "* Categorical variables can be tricky. Youâ€™ll explore:\n",
    "\n",
    "  * One-hot encoding (works well with trees)\n",
    "  * Label encoding\n",
    "  * **Target encoding** (advanced; encode categories based on target value)\n",
    "  * Cardinality issues (too many unique categories)\n",
    "\n",
    "```python\n",
    "# One-hot encoding\n",
    "data = pd.get_dummies(data, columns=['neighborhood'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Feature Generation**\n",
    "\n",
    "* How to **create new features** from dates, text, and domain knowledge.\n",
    "* Examples:\n",
    "\n",
    "  * Extracting **day of week** from a date\n",
    "  * Counting **number of words** in a text\n",
    "  * Binning numeric variables into categories (e.g. age groups)\n",
    "\n",
    "```python\n",
    "data['year'] = data['date'].dt.year\n",
    "data['name_length'] = data['name'].apply(len)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Feature Selection**\n",
    "\n",
    "* Not all features help â€” some hurt.\n",
    "* Youâ€™ll learn to:\n",
    "\n",
    "  * Use **correlation** and **importance scores** to drop useless features\n",
    "  * Use `Permutation Importance` to see how much each feature affects model performance\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import permutation_importance\n",
    "results = permutation_importance(model, X_val, y_val)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Skills You'll Gain\n",
    "\n",
    "* Create new, meaningful features from raw data\n",
    "* Transform features for better model learning\n",
    "* Use feature importance to prune or prioritize features\n",
    "* Improve model accuracy by using better input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55018f65",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
